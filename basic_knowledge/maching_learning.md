**神经网络**

1. [梯度更新算法-owlqn](http://www.bigbear2017.com/blog/2016/06/07/owl-qnsuan-fa-jie-shao/)  
2. [梯度更新算法-Momentum,AdaGrad,RMSProp](https://blog.csdn.net/u014595019/article/details/52989301)
3. [激活函数+损失函数](https://www.cnblogs.com/pinard/p/6437495.html)

---

**GBDT, xgboost**
1. [GBDT总结](https://my.oschina.net/SearchVera/blog/1591457)
2. [xgboost-公式推导](https://www.zybuluo.com/yxd/note/611571)
3. [xgboost-与GBDT区别](https://www.jianshu.com/p/af1fbcd6058d) 
4. [xgboost-参数调优](https://blog.csdn.net/u010657489/article/details/51952785)
5. [xgboost-运行方法](https://www.jianshu.com/p/7e0e2d66b3d4)
6. [xgboost-缺失值处理](https://www.zhihu.com/question/34867991) 

---

**Logistic Regression** 
1. [LR-原理](https://blog.csdn.net/cyh_24/article/details/50359055)
2. [LR-和softmax关系](http://www.cnblogs.com/maybe2030/p/5678387.html)
2. [LR-和朴素贝叶斯的区别](https://blog.csdn.net/chlele0105/article/details/38922551)

---

**聚类**
1. [聚类-kmeans](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html)
2. [聚类-dbscan](http://www.cnblogs.com/chaosimple/p/3164775.html)
3. [聚类-Louvain](http://www.cnblogs.com/fengfenggirl/p/louvain.html)

---

**其他**
1. [geohash](http://www.cnblogs.com/LBSer/p/3310455.html) 
2. [生成模型vs判别模型](http://www.cnblogs.com/ranjiewen/articles/6736640.html)
3. [线性回归，最小二乘](http://pytlab.org/2017/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-%E6%A0%87%E5%87%86%E4%B8%8E%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#)
4. [why最小二乘](https://www.zhihu.com/question/24095027/answer/30763880)
